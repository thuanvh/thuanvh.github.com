---
layout: post
title: Brief SGD
date: 2019-11-18 08:23:44.000000000 +07:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
---

Post about some SGDs [here](
https://www.datasciencecentral.com/profiles/blogs/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent
)

In the above reference, the author displays some idea of some SGD optimization in a short way which support us to summarize important ideas of SGD.

# SGD

# Gradient Perturbation
Add small noisy term to gradient.

# Momentum and Nesterov Momentum
Add a correction factor (a history of gradient change, exponentially weighted moving average) to gradient.

# RMSProp : 
Adapt correction factor to each parameter.


