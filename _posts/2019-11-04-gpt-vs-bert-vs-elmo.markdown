---
layout: post
title: GPT vs BERT vs ELMo
date: 2019-11-04 07:27:34.000000000 +07:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories: []
tags: []
meta:
  timeline_notification: '1572852457'
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '37055326282'

permalink: "/2019/11/04/gpt-vs-bert-vs-elmo/"
---

For language model, I read a slide for fast reading GPT, BERT and ELMo.
https://52paper.github.io/20181018_gaojun.pdf

<img class="alignnone size-full wp-image-34" src="{{ site.baseurl }}/assets/untitled.png" alt="Untitled" width="989" height="426" />

For detail in BERT transformer based on attentions, there is a blog that shows how BERT attention show the parsing tree

https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db

The visualization tool for understanding attentions:

https://github.com/jessevig/bertviz

And a video for bertviz:

There is a list of pretrained language models provided by HuggingFace Transformers:

https://huggingface.co/transformers/index.html




