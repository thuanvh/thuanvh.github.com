---
layout: post
title: GPT vs BERT vs ELMo
date: 2019-11-04 07:27:34.000000000 +07:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories: []
tags: []
meta:
  timeline_notification: '1572852457'
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '37055326282'

permalink: "/2019/11/04/gpt-vs-bert-vs-elmo/"
---
<p>For language model, I read a slide for fast reading GPT, BERT and ELMo.</p>
<p><a href="https://52paper.github.io/20181018_gaojun.pdf">https://52paper.github.io/20181018_gaojun.pdf<br />
</a><br />
<img class="alignnone size-full wp-image-34" src="{{ site.baseurl }}/assets/untitled.png" alt="Untitled" width="989" height="426" /></p>
<p>For detail in BERT transformer based on attentions, there is a blog that shows how BERT attention show the parsing tree</p>
<p><a href="https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db">https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db</a></p>
<p>The visualization tool for understanding attentions:</p>
<p><a href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</a></p>
<p>And a video for bertviz:</p>
<p>There is a list of pretrained language models provided by HuggingFace Transformers:</p>
<p><a href="https://huggingface.co/transformers/index.html">https://huggingface.co/transformers/index.html</a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
