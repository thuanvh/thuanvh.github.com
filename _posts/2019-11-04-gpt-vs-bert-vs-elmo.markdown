---
layout: post
title: GPT vs BERT vs ELMo
date: 2019-11-04 07:27:34.000000000 +07:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories: []

---

For language model, I read a slide for fast reading GPT, BERT and ELMo.
[Document pdf](
https://52paper.github.io/20181018_gaojun.pdf)

For detail in BERT transformer based on attentions, there is a blog that shows how BERT attention show the parsing tree

<https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db>

The visualization tool for understanding attentions:

<https://github.com/jessevig/bertviz>

And a video for bertviz:

There is a list of pretrained language models provided by HuggingFace Transformers:

<https://huggingface.co/transformers/index.html>

